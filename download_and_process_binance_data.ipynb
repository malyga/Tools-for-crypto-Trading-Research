{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import os.path\n",
    "import glob\n",
    "import time\n",
    "from binance.client import Client\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil import parser\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define fixed arguments to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/vladimirmalygin/Desktop/OneDrive/Lemniscate Capital/Data/Crypto/1m/') # set a directory where you would like to store the data\n",
    "binance_api_key = '[]' # set your api key\n",
    "binance_api_secret = '[]' # set your api secret key\n",
    "binsizes = {\"1m\": 1, \"5m\": 5, \"1h\": 60, \"1d\": 1440}\n",
    "batch_size = 750\n",
    "binance_client = Client(api_key=binance_api_key, \n",
    "                        api_secret=binance_api_secret) # get access to Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minutes_of_new_data(symbol, kline_size, data, source):\n",
    "    if len(data) > 0:\n",
    "        old = parser.parse(data[\"timestamp\"].iloc[-1])\n",
    "    elif source == \"binance\":\n",
    "        old = datetime.datetime.strptime('1 Jan 2017', '%d %b %Y')\n",
    "    if source == \"binance\":\n",
    "        new = pd.to_datetime(binance_client.get_klines(symbol=symbol,\n",
    "                                                       interval=kline_size)[-1][0], unit='ms')\n",
    "    return old, new\n",
    "\n",
    "def downloadAllBinance(symbol, kline_size, save=False):\n",
    "    filename = '%s-%s-data.csv' % (symbol, kline_size)\n",
    "    if os.path.isfile(filename):\n",
    "        data_df = pd.read_csv(filename)\n",
    "    else:\n",
    "        data_df = pd.DataFrame()\n",
    "    oldest_point, newest_point = minutes_of_new_data(\n",
    "        symbol, kline_size, data_df, source=\"binance\")\n",
    "    delta_min = (newest_point - oldest_point).total_seconds() / 60\n",
    "    available_data = math.ceil(delta_min / binsizes[kline_size])\n",
    "    if oldest_point == datetime.datetime.strptime('1 Jan 2017', '%d %b %Y'):\n",
    "        print('Downloading all available %s data for %s.' %\n",
    "              (kline_size, symbol))\n",
    "    else:\n",
    "        print('Downloading %d minutes of new data available for %s, i.e. %d instances of %s data.' % (\n",
    "            delta_min, symbol, available_data, kline_size))\n",
    "    klines = binance_client.get_historical_klines(symbol, kline_size, oldest_point.strftime(\n",
    "        \"%d %b %Y %H:%M:%S\"), newest_point.strftime(\"%d %b %Y %H:%M:%S\"))\n",
    "    data = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close',\n",
    "                                         'volume', 'close_time', 'quote_av', 'trades',\n",
    "                                         'tb_base_av', 'tb_quote_av', 'ignore'])\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')\n",
    "    if len(data_df) > 0:\n",
    "        temp_df = pd.DataFrame(data)\n",
    "        data_df = data_df.append(temp_df)\n",
    "    else:\n",
    "        data_df = data\n",
    "    data_df.set_index('timestamp', inplace=True)\n",
    "    if save:\n",
    "        data_df.to_csv(filename)\n",
    "    # print('All caught up..!')\n",
    "    return data_df\n",
    "\n",
    "def find_symbol_filenames(directory_to_raw_data, \n",
    "                          tickers_to_process = None, \n",
    "                          base_ticker = \"USDT\",\n",
    "                          frequency = \"1d\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    The function is created to find necessary price data among a variety\n",
    "    of cryptopairs in `directory_to_raw_data`. It is possible to specify\n",
    "    cryptopairs via `tickers_to_process` or define `base_ticker` which is used\n",
    "    to find all cryptopairs that are traded to `base_ticker` (e.g., given\n",
    "    `base_ticker` = \"USDT\", the \"BTCUSDT\" pair will be found). \n",
    "    The function requires that filenames that are stored in `directory_to_raw_data`\n",
    "    to follow the following name pattern: \"Ticker-frequency-data.csv\". For instance,\n",
    "    \"BTCUSDT-1d-data.csv\".\n",
    "    \n",
    "    Arguments:\n",
    "        directory_to_raw_data -- string, path to directory where price data is stored\n",
    "        tickers_to_process -- list, the data for the tickers in the list will be searched (e.g., [\"BTCUSDT\", \"XRPBTC\"])\n",
    "        base_ticker -- string, all pairs associated with this ticker will be searched (e.g., [\"BTCUSDT\", \"ETHUSDT\"])\n",
    "        frequency -- string, the data frequency of a file to search for (e.g., \"1m\", \"1d\", etc.)\n",
    "        tickers_to_process -- list of tickers that were searched (auxiliary)\n",
    "        selected_file_names -- list of filenames found\n",
    "    \"\"\"\n",
    "    \n",
    "    path = directory_to_raw_data + \"*\" + \".csv\"\n",
    "    all_file_names = glob.glob(path)\n",
    "    all_tickers = [file_name.split('-')[0].split('/')[-1] for file_name in all_file_names]\n",
    "    usdt_tickers = []\n",
    "    btc_tickers = []\n",
    "    for i in range(0, len(all_tickers)):\n",
    "        try:\n",
    "            if(isinstance(all_tickers[i].index(\"USDT\"), int) == True):\n",
    "                usdt_tickers.append(all_tickers[i])\n",
    "        except:\n",
    "            if(isinstance(all_tickers[i].index(\"BTC\"), int) == True):\n",
    "                btc_tickers.append(all_tickers[i])\n",
    "    if (tickers_to_process == None) & (base_ticker == \"USDT\"):\n",
    "        tickers_to_process = usdt_tickers\n",
    "        \n",
    "    elif (tickers_to_process == None) & (base_ticker == \"BTC\"):\n",
    "        tickers_to_process = btc_tickers\n",
    "    \n",
    "    selected_file_names =[directory_to_raw_data\n",
    "                             + ticker + \"-\"\n",
    "                             + frequency + \n",
    "                             \"-data\"\n",
    "                             + '.csv'\n",
    "                             for ticker in tickers_to_process]\n",
    "    return(tickers_to_process, selected_file_names)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 320413 minutes of new data available for PNTUSDT, i.e. 320413 instances of 1m data.\n"
     ]
    }
   ],
   "source": [
    "tickers, filenames  = find_symbol_filenames(directory_to_raw_data = '')\n",
    "  \n",
    "# tickers = [\"BTCUSDT\"] # symbols to download\n",
    "timeElapsed = [] # stores time elampsed for each item\n",
    "tickers_missed = [] # stores the tickers that were not dowloaded for troubleshoting\n",
    "numOfTickers = len(tickers) # total number of tickers to download\n",
    "iterations = numOfTickers # iterations left\n",
    "freq = \"1m\" # data frequency {\"1m\": 1, \"5m\": 5, \"1h\": 60, \"1d\": 1440}\n",
    "\n",
    "for symbol in tickers:\n",
    "\n",
    "    try:\n",
    "        startTimer = time.time()\n",
    "        downloadAllBinance(symbol, freq, save=True)\n",
    "        endTimer = time.time()\n",
    "        timeElapsedPoint = endTimer - startTimer\n",
    "        iterations += -1\n",
    "        timeElapsed.append(timeElapsedPoint)\n",
    "        expetedTimeLeftMinutes = round(np.mean(np.array(timeElapsed)) * iterations / 60, 1)\n",
    "        percentageUploaded = round((1 - iterations / numOfTickers) * 100, 1)\n",
    "        print('All available data for ' + symbol + ' downloaded.')\n",
    "        print(str(percentageUploaded) + \"% \" + \"downloaded/updated. The expected time to\"\n",
    "              + \" complete is equal to \" + str(expetedTimeLeftMinutes)\n",
    "              + \" minutes.\")\n",
    "    except:\n",
    "        tickers_missed.append(symbol)\n",
    "        print(symbol + \" \" + \"was not downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions to download OHLCV tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        \n",
    "def Average(lst):\n",
    "\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "def generate_dates_vector(start_date, end_date, step = 60*60*24):\n",
    "    \n",
    "    \"\"\"\n",
    "    The function generates a sequence of dates with a fixed step (seconds).\n",
    "    \n",
    "    Dependencies: datetime and pandas packages\n",
    "     Arguments:\n",
    "        start_date -- datetime object. Example: datetime.datetime(2018, 1, 1, 0, 00, 00) # '%Y-%m-%d %H:%M:%S'       \n",
    "        end_date -- datetime object. Example: datetime.datetime(2021, 1, 1, 0, 00, 00) # '%Y-%m-%d %H:%M:%S'    \n",
    "    Returns:\n",
    "        vectorDates -- pandas data frame with 1 column and number of rows equal to the number of periods.\n",
    "    \"\"\"\n",
    "    \n",
    "    step = timedelta(seconds = step)\n",
    "    startDate = start_date\n",
    "    endDate = end_date\n",
    "\n",
    "    vectorDates = []\n",
    "\n",
    "    while startDate < endDate:\n",
    "        vectorDates.append(startDate.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        startDate += step\n",
    "    vectorDates = pd.DataFrame(np.asanyarray(vectorDates, dtype='datetime64'))\n",
    "    vectorDates.rename(columns = {0: \"Date\"}, inplace = True)\n",
    "    return(vectorDates)\n",
    "        \n",
    "def create_OCHLVT_tables(start_date, end_date, \n",
    "                         step, directory_to_raw_data, \n",
    "                         export_directory, \n",
    "                         tickers_to_process = None, \n",
    "                         base_ticker = \"USDT\"):\n",
    "    directory_to_raw_data = directory_to_raw_data\n",
    "    columnIndexes = [1, 2, 3, 4, 5, 8]\n",
    "    columnNames = ['open', 'high', 'low', 'close', 'volume', 'trades']\n",
    "    vectorDates = generate_dates_vector(start_date = start_date,\n",
    "                                        end_date = end_date, step = step)\n",
    "                                        \n",
    "    tickers, relevant_file_names  = find_symbol_filenames(directory_to_raw_data, \n",
    "                                                          tickers_to_process, \n",
    "                                                          base_ticker)\n",
    "    fileNames = relevant_file_names\n",
    "    iterations = len(columnIndexes)\n",
    "    timeElapsed = []\n",
    "    coinFlag = base_ticker\n",
    "    indexForColumnNames = 0\n",
    "    for columnIndex in columnIndexes:\n",
    "        startTimer = time.time()\n",
    "        finalTable = None\n",
    "        finalTable = np.empty(shape=(len(vectorDates), len(fileNames)),\n",
    "                              dtype='float')\n",
    "        i = 0\n",
    "        for fileName in fileNames:\n",
    "            dataFrame = pd.read_csv(fileName, usecols=[0, columnIndex])\n",
    "            ochlvFlag = dataFrame.columns[1]\n",
    "            targetColumn = np.asanyarray(dataFrame.iloc[:, 1], dtype='float')\n",
    "            timeStampsVector = np.asanyarray(dataFrame['timestamp'],\n",
    "                                             dtype='datetime64')\n",
    "            foundTimeStamps, indexIntersectBasis, indexIntersectLocal = \\\n",
    "                np.intersect1d(vectorDates, timeStampsVector, return_indices=True)\n",
    "            finalTable[indexIntersectBasis, i] = targetColumn[indexIntersectLocal]\n",
    "            i += 1\n",
    "        finalTable[finalTable == 0] = np.nan\n",
    "        finalTableDataFrame = pd.DataFrame(finalTable)\n",
    "        finalTableDataFrame = pd.concat([vectorDates,\n",
    "                                         finalTableDataFrame], axis=1)\n",
    "        finalTableDataFrame.columns = ['Date'] + tickers\n",
    "        star_date_name = str(start_date.year) + \"-\" \\\n",
    "                         + str(start_date.month) + \"-\" \\\n",
    "                         + str(start_date.day)\n",
    "        end_date_name = str(end_date.year) + \"-\" \\\n",
    "                        + str(end_date.month) + \"-\" \\\n",
    "                        + str(end_date.day)\n",
    "        fileNameToWrite = star_date_name + '_' \\\n",
    "                          + end_date_name + '-' \\\n",
    "                          + ochlvFlag + '-' \\\n",
    "                          + coinFlag + '.csv'\n",
    "        \n",
    "        finalTableDataFrame.to_csv(export_directory + fileNameToWrite, index=False)\n",
    "        endTimer = time.time()\n",
    "        timeElapsedPoint = endTimer - startTimer\n",
    "        iterations += -1\n",
    "        timeElapsed.append(timeElapsedPoint)\n",
    "        expetedTimeLeftMinutes = round(Average(timeElapsed) * iterations / 60, 2)\n",
    "        finalTableDataFrame = None\n",
    "        indexForColumnNames += 1\n",
    "\n",
    "        print(ochlvFlag + \" data\" + \" has been generated. \" + str(iterations)\n",
    "              + \" files to generate left. \" + \"Expected time to complete: \"\n",
    "              + str(expetedTimeLeftMinutes) + \" minutes.\")\n",
    "\n",
    "        print('The name of the generated file is ' + fileNameToWrite + '.')\n",
    "        print()\n",
    "\n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate OHLCV tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open data has been generated. 5 files to generate left. Expected time to complete: 0.0 minutes.\n",
      "The name of the generated file is 2018-1-1_2021-2-15-open-USDT.csv.\n",
      "\n",
      "high data has been generated. 4 files to generate left. Expected time to complete: 0.0 minutes.\n",
      "The name of the generated file is 2018-1-1_2021-2-15-high-USDT.csv.\n",
      "\n",
      "low data has been generated. 3 files to generate left. Expected time to complete: 0.0 minutes.\n",
      "The name of the generated file is 2018-1-1_2021-2-15-low-USDT.csv.\n",
      "\n",
      "close data has been generated. 2 files to generate left. Expected time to complete: 0.0 minutes.\n",
      "The name of the generated file is 2018-1-1_2021-2-15-close-USDT.csv.\n",
      "\n",
      "volume data has been generated. 1 files to generate left. Expected time to complete: 0.0 minutes.\n",
      "The name of the generated file is 2018-1-1_2021-2-15-volume-USDT.csv.\n",
      "\n",
      "trades data has been generated. 0 files to generate left. Expected time to complete: 0.0 minutes.\n",
      "The name of the generated file is 2018-1-1_2021-2-15-trades-USDT.csv.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date_input = datetime.datetime(2018, 1, 1, 0, 00, 00) # '%Y-%m-%d %H:%M:%S'\n",
    "end_date_input = datetime.datetime(2021, 2, 15, 12, 00, 00) # '%Y-%m-%d %H:%M:%S'\n",
    "directory_to_raw_data = '/Users/vladimirmalygin/Desktop/test/crypto_pairs/'\n",
    "directory_export = '/Users/vladimirmalygin/Desktop/test/processed_data/'\n",
    "path = directory_to_raw_data + \"*\" + \".csv\"\n",
    "all_file_names = glob.glob(path)\n",
    "\n",
    "create_OCHLVT_tables(start_date = start_date_input, \n",
    "                     end_date = end_date_input, \n",
    "                     step = 60*60*24,\n",
    "                     directory_to_raw_data = directory_to_raw_data,\n",
    "                     export_directory = directory_export,\n",
    "                     tickers_to_process = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
